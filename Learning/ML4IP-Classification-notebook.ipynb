{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "df1 = pd.read_csv('F_List1.csv',delimiter=' ',header=None)\n",
    "df2 = pd.read_csv('F_List2.csv',delimiter=' ',header=None)\n",
    "df3 = pd.read_csv('F_List3.csv',delimiter=' ',header=None)\n",
    "df4 = pd.read_csv('F_List4.csv',delimiter=' ',header=None)\n",
    "df5 = pd.read_csv('F_List5.csv',delimiter=' ',header=None)\n",
    "df6 = pd.read_csv('F_List6.csv',delimiter=' ',header=None)\n",
    "df7 = pd.read_csv('F_List7.csv',delimiter=' ',header=None)\n",
    "df8 = pd.read_csv('F_List8.csv',delimiter=' ',header=None)\n",
    "df9 = pd.read_csv('F_List9.csv',delimiter=' ',header=None)\n",
    "df10 = pd.read_csv('F_List10.csv',delimiter=' ',header=None)\n",
    "df11 = pd.read_csv('F_List11.csv',delimiter=' ',header=None)\n",
    "df12 = pd.read_csv('F_List12.csv',delimiter=' ',header=None)\n",
    "df13 = pd.read_csv('F_List13.csv',delimiter=' ',header=None)\n",
    "df14 = pd.read_csv('F_List14.csv',delimiter=' ',header=None)\n",
    "df15 = pd.read_csv('F_List15.csv',delimiter=' ',header=None)\n",
    "df16 = pd.read_csv('F_List16.csv',delimiter=' ',header=None)\n",
    "df17 = pd.read_csv('F_List17.csv',delimiter=' ',header=None)\n",
    "df18 = pd.read_csv('F_List18.csv',delimiter=' ',header=None)\n",
    "df19 = pd.read_csv('F_List19.csv',delimiter=' ',header=None)\n",
    "df20 = pd.read_csv('F_List20.csv',delimiter=' ',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "df_new1 = pd.read_csv('F_raw_rawL1.csv',delimiter=' ',header=None)\n",
    "df_new2 = pd.read_csv('F_raw_rawL2.csv',delimiter=' ',header=None)\n",
    "df_new3 = pd.read_csv('F_raw_rawL3.csv',delimiter=' ',header=None)\n",
    "df_new4 = pd.read_csv('F_raw_rawL4.csv',delimiter=' ',header=None)\n",
    "df_new5 = pd.read_csv('F_raw_rawL5.csv',delimiter=' ',header=None)\n",
    "df_new6 = pd.read_csv('F_raw_rawL6.csv',delimiter=' ',header=None)\n",
    "df_new7 = pd.read_csv('F_raw_rawL7.csv',delimiter=' ',header=None)\n",
    "df_new8 = pd.read_csv('F_raw_rawL8.csv',delimiter=' ',header=None)\n",
    "df_new9 = pd.read_csv('F_raw_rawL9.csv',delimiter=' ',header=None)\n",
    "df_new10 = pd.read_csv('F_raw_rawL10.csv',delimiter=' ',header=None)\n",
    "df_new11 = pd.read_csv('F_raw_rawL11.csv',delimiter=' ',header=None)\n",
    "df_new12 = pd.read_csv('F_raw_rawL12.csv',delimiter=' ',header=None)\n",
    "df_new13 = pd.read_csv('F_raw_rawL13.csv',delimiter=' ',header=None)\n",
    "df_new14 = pd.read_csv('F_raw_rawL14.csv',delimiter=' ',header=None)\n",
    "df_new15 = pd.read_csv('F_raw_rawL15.csv',delimiter=' ',header=None)\n",
    "df_new16 = pd.read_csv('F_raw_rawL16.csv',delimiter=' ',header=None)\n",
    "df_new17 = pd.read_csv('F_raw_rawL17.csv',delimiter=' ',header=None)\n",
    "df_new18 = pd.read_csv('F_raw_rawL18.csv',delimiter=' ',header=None)\n",
    "df_new19 = pd.read_csv('F_raw_rawL19.csv',delimiter=' ',header=None)\n",
    "df_new20 = pd.read_csv('F_raw_rawL20.csv',delimiter=' ',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge all these dataframes in a big dataframe\n",
    "List1 = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20]\n",
    "List2 = [df_new1,df_new2,df_new3,df_new4,df_new5,df_new6,df_new7,df_new8,df_new9,df_new10,df_new11,df_new12,df_new13,df_new14,df_new15,df_new16,df_new17,df_new18,df_new19,df_new20]\n",
    "List = List1 + List2\n",
    "df = pd.concat(List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 10]\n",
      "[13, 16]\n"
     ]
    }
   ],
   "source": [
    "# Check for inf or -inf values\n",
    "Min = []\n",
    "Max = []\n",
    "i=1\n",
    "while i<83:\n",
    "    if min(df[i])==-np.inf:\n",
    "        Min.append(i)\n",
    "    if max(df[i])==np.inf:\n",
    "        Max.append(i)\n",
    "    i+=1\n",
    "print(Min)\n",
    "print(Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the first column and change the header name\n",
    "header = []\n",
    "i=1\n",
    "while i<83:\n",
    "    header.append(i)\n",
    "    i+=1\n",
    "df=df.drop(df.columns[0],axis=1)\n",
    "df.columns=header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model 1: Change the target beta to 1 when it is strictly greater than 0\n",
    "\"\"\"\n",
    "# Drop those columns with inf value\n",
    "df1=df.replace([np.inf, -np.inf], np.nan)\n",
    "df1=df1.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df1[82] = (df1[82] > 0).astype(int)\n",
    "X1 = df1.iloc[:,0:77]\n",
    "y1 = df1[82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and test sets\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, train_size=train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9828458942632171\n",
      "Test Accuracy  ::  0.6687289088863893\n",
      "Kappa score  ::  0.33055025066601895\n",
      " Confusion matrix  [[1427  434]\n",
      " [ 744  951]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.77      0.71      1861\n",
      "          1       0.69      0.56      0.62      1695\n",
      "\n",
      "avg / total       0.67      0.67      0.66      3556\n",
      "\n",
      "Feature ranking:\n",
      "1. feature 14 (0.036283)\n",
      "2. feature 21 (0.036069)\n",
      "3. feature 20 (0.034655)\n",
      "4. feature 50 (0.033247)\n",
      "5. feature 28 (0.031021)\n",
      "6. feature 24 (0.029226)\n",
      "7. feature 26 (0.028929)\n",
      "8. feature 29 (0.028331)\n",
      "9. feature 76 (0.027150)\n",
      "10. feature 66 (0.027038)\n",
      "11. feature 55 (0.025298)\n",
      "12. feature 75 (0.024833)\n",
      "13. feature 68 (0.024061)\n",
      "14. feature 61 (0.023983)\n",
      "15. feature 3 (0.023720)\n",
      "16. feature 11 (0.023592)\n",
      "17. feature 25 (0.023484)\n",
      "18. feature 16 (0.023456)\n",
      "19. feature 9 (0.023353)\n",
      "20. feature 17 (0.023301)\n",
      "21. feature 5 (0.022973)\n",
      "22. feature 1 (0.022562)\n",
      "23. feature 2 (0.022553)\n",
      "24. feature 22 (0.022316)\n",
      "25. feature 18 (0.022081)\n",
      "26. feature 12 (0.021676)\n",
      "27. feature 6 (0.021117)\n",
      "28. feature 10 (0.020883)\n",
      "29. feature 7 (0.020582)\n",
      "30. feature 8 (0.020283)\n",
      "31. feature 13 (0.019741)\n",
      "32. feature 72 (0.011672)\n",
      "33. feature 71 (0.010348)\n",
      "34. feature 35 (0.009921)\n",
      "35. feature 41 (0.009454)\n",
      "36. feature 34 (0.009439)\n",
      "37. feature 38 (0.009143)\n",
      "38. feature 31 (0.009112)\n",
      "39. feature 32 (0.008883)\n",
      "40. feature 33 (0.008859)\n",
      "41. feature 73 (0.008848)\n",
      "42. feature 39 (0.008616)\n",
      "43. feature 30 (0.008546)\n",
      "44. feature 0 (0.007888)\n",
      "45. feature 51 (0.007268)\n",
      "46. feature 40 (0.007177)\n",
      "47. feature 67 (0.007012)\n",
      "48. feature 54 (0.006725)\n",
      "49. feature 37 (0.006344)\n",
      "50. feature 44 (0.006319)\n",
      "51. feature 63 (0.006117)\n",
      "52. feature 69 (0.005711)\n",
      "53. feature 49 (0.005316)\n",
      "54. feature 43 (0.005281)\n",
      "55. feature 42 (0.005006)\n",
      "56. feature 70 (0.004917)\n",
      "57. feature 36 (0.004670)\n",
      "58. feature 46 (0.004550)\n",
      "59. feature 52 (0.004094)\n",
      "60. feature 45 (0.003634)\n",
      "61. feature 65 (0.001043)\n",
      "62. feature 15 (0.000292)\n",
      "63. feature 4 (0.000000)\n",
      "64. feature 62 (0.000000)\n",
      "65. feature 74 (0.000000)\n",
      "66. feature 64 (0.000000)\n",
      "67. feature 56 (0.000000)\n",
      "68. feature 60 (0.000000)\n",
      "69. feature 59 (0.000000)\n",
      "70. feature 58 (0.000000)\n",
      "71. feature 57 (0.000000)\n",
      "72. feature 53 (0.000000)\n",
      "73. feature 23 (0.000000)\n",
      "74. feature 48 (0.000000)\n",
      "75. feature 47 (0.000000)\n",
      "76. feature 27 (0.000000)\n",
      "77. feature 19 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# Train the random forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "#Feature ranking\n",
    "forest = ExtraTreesClassifier(n_estimators=78,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.6912260967379078\n",
      "0.7574241653312284\n",
      "[[1146  715]\n",
      " [ 383 1312]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.77      0.71      1861\n",
      "          1       0.69      0.56      0.62      1695\n",
      "\n",
      "avg / total       0.67      0.67      0.66      3556\n",
      "\n",
      "Kappa score  ::  0.33055025066601895\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression classifier\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tSVM Result\n",
      "0.5672103487064117\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.77      0.71      1861\n",
      "          1       0.69      0.56      0.62      1695\n",
      "\n",
      "avg / total       0.67      0.67      0.66      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM classifier\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.6296400449943758\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.67      0.65      1861\n",
      "          1       0.62      0.59      0.60      1695\n",
      "\n",
      "avg / total       0.63      0.63      0.63      3556\n",
      "\n",
      "The optimal number of neighbors is 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx05JCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b5bcb9d0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(myList, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.64      0.66      1861\n",
      "          1       0.63      0.66      0.64      1695\n",
      "\n",
      "avg / total       0.65      0.65      0.65      3556\n",
      "\n",
      "[[1200  661]\n",
      " [ 582 1113]]\n",
      "Kappa score  ::  0.3008264598766448\n"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tNeural Network Result\n",
      "[[1296  565]\n",
      " [ 700  995]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.70      0.67      1861\n",
      "          1       0.64      0.59      0.61      1695\n",
      "\n",
      "avg / total       0.64      0.64      0.64      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "#Scale the feature\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 2: Change the target beta to 1 when beta is greater than 0.05\n",
    "\"\"\"\n",
    "df2=df.replace([np.inf, -np.inf], np.nan)\n",
    "df2=df2.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df2[82] = (df2[82] > 0.05).astype(int)\n",
    "X = df2.iloc[:,0:77]\n",
    "y = df2[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9868766404199475\n",
      "Test Accuracy  ::  0.8892013498312711\n",
      " Confusion matrix  [[3127   59]\n",
      " [ 335   35]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      3186\n",
      "          1       0.37      0.09      0.15       370\n",
      "\n",
      "avg / total       0.85      0.89      0.86      3556\n",
      "\n",
      "Feature ranking:\n",
      "1. feature 20 (0.037278)\n",
      "2. feature 14 (0.035724)\n",
      "3. feature 50 (0.034160)\n",
      "4. feature 21 (0.032916)\n",
      "5. feature 28 (0.031879)\n",
      "6. feature 26 (0.028847)\n",
      "7. feature 66 (0.028802)\n",
      "8. feature 76 (0.026666)\n",
      "9. feature 16 (0.025960)\n",
      "10. feature 1 (0.025412)\n",
      "11. feature 2 (0.025378)\n",
      "12. feature 3 (0.025263)\n",
      "13. feature 29 (0.025048)\n",
      "14. feature 12 (0.024137)\n",
      "15. feature 9 (0.024041)\n",
      "16. feature 17 (0.023888)\n",
      "17. feature 25 (0.023765)\n",
      "18. feature 75 (0.023521)\n",
      "19. feature 22 (0.023481)\n",
      "20. feature 11 (0.023241)\n",
      "21. feature 6 (0.023141)\n",
      "22. feature 18 (0.023053)\n",
      "23. feature 55 (0.023005)\n",
      "24. feature 68 (0.022960)\n",
      "25. feature 10 (0.022702)\n",
      "26. feature 5 (0.022702)\n",
      "27. feature 61 (0.022644)\n",
      "28. feature 7 (0.022181)\n",
      "29. feature 8 (0.021599)\n",
      "30. feature 13 (0.021579)\n",
      "31. feature 24 (0.017352)\n",
      "32. feature 72 (0.012370)\n",
      "33. feature 71 (0.010429)\n",
      "34. feature 35 (0.010049)\n",
      "35. feature 0 (0.010030)\n",
      "36. feature 34 (0.009822)\n",
      "37. feature 41 (0.009736)\n",
      "38. feature 33 (0.009220)\n",
      "39. feature 32 (0.009125)\n",
      "40. feature 39 (0.009004)\n",
      "41. feature 31 (0.008771)\n",
      "42. feature 30 (0.008499)\n",
      "43. feature 38 (0.008439)\n",
      "44. feature 54 (0.007281)\n",
      "45. feature 37 (0.006994)\n",
      "46. feature 73 (0.006360)\n",
      "47. feature 44 (0.006018)\n",
      "48. feature 70 (0.005882)\n",
      "49. feature 51 (0.005573)\n",
      "50. feature 63 (0.005569)\n",
      "51. feature 49 (0.005220)\n",
      "52. feature 42 (0.005051)\n",
      "53. feature 43 (0.005032)\n",
      "54. feature 69 (0.004811)\n",
      "55. feature 36 (0.004696)\n",
      "56. feature 45 (0.004623)\n",
      "57. feature 46 (0.004592)\n",
      "58. feature 40 (0.004281)\n",
      "59. feature 52 (0.004236)\n",
      "60. feature 67 (0.004154)\n",
      "61. feature 65 (0.001070)\n",
      "62. feature 15 (0.000736)\n",
      "63. feature 53 (0.000000)\n",
      "64. feature 74 (0.000000)\n",
      "65. feature 27 (0.000000)\n",
      "66. feature 47 (0.000000)\n",
      "67. feature 4 (0.000000)\n",
      "68. feature 48 (0.000000)\n",
      "69. feature 64 (0.000000)\n",
      "70. feature 19 (0.000000)\n",
      "71. feature 23 (0.000000)\n",
      "72. feature 62 (0.000000)\n",
      "73. feature 60 (0.000000)\n",
      "74. feature 59 (0.000000)\n",
      "75. feature 58 (0.000000)\n",
      "76. feature 57 (0.000000)\n",
      "77. feature 56 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# Train the random forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "forest = ExtraTreesClassifier(n_estimators=78,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.8956692913385826\n",
      "0.8174988547869904\n",
      "[[3185    1]\n",
      " [ 370    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      3186\n",
      "          1       0.37      0.09      0.15       370\n",
      "\n",
      "avg / total       0.85      0.89      0.86      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression classifier\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tSVM Result\n",
      "0.8942632170978627\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      3186\n",
      "          1       0.37      0.09      0.15       370\n",
      "\n",
      "avg / total       0.85      0.89      0.86      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM classifier\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.8945444319460067\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.94      3186\n",
      "          1       0.22      0.01      0.01       370\n",
      "\n",
      "avg / total       0.83      0.89      0.85      3556\n",
      "\n",
      "The optimal number of neighbors is 47\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b5bcbcb470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(myList, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.62      0.75      3186\n",
      "          1       0.19      0.78      0.31       370\n",
      "\n",
      "avg / total       0.88      0.63      0.71      3556\n",
      "\n",
      "[[1967 1219]\n",
      " [  82  288]]\n",
      "Kappa score  ::  0.16783657035466937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tNeural Network Result\n",
      "[[2959  227]\n",
      " [ 261  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.93      0.92      3186\n",
      "          1       0.32      0.29      0.31       370\n",
      "\n",
      "avg / total       0.86      0.86      0.86      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "#Scale the feature\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tLinear Regression Result\n",
      "Mean squared error: 0.00\n",
      "R squared: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 3: Linear Regression\n",
    "\"\"\"\n",
    "df3=df.replace([np.inf, -np.inf], np.nan)\n",
    "df3=df3.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "X = df3.iloc[:,0:77]\n",
    "y = df3[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "# The mean squared error\n",
    "print (\"\\t\\t\\t\\t\\tLinear Regression Result\")\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('R squared: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9863431151241535\n",
      "Test Accuracy  ::  0.8767772511848341\n",
      " Confusion matrix  [[2504   71]\n",
      " [ 293   86]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.97      0.93      2575\n",
      "          1       0.55      0.23      0.32       379\n",
      "\n",
      "avg / total       0.85      0.88      0.85      2954\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.8716993906567366\n",
      "0.8768973025591105\n",
      "[[2575    0]\n",
      " [ 379    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.97      0.93      2575\n",
      "          1       0.55      0.23      0.32       379\n",
      "\n",
      "avg / total       0.85      0.88      0.85      2954\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.8740690589031821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.97      0.93      2575\n",
      "          1       0.55      0.23      0.32       379\n",
      "\n",
      "avg / total       0.85      0.88      0.85      2954\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.8747461069735951\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.93      2575\n",
      "          1       0.68      0.04      0.08       379\n",
      "\n",
      "avg / total       0.85      0.87      0.82      2954\n",
      "\n",
      "The optimal number of neighbors is 49\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.67      0.79      2575\n",
      "          1       0.27      0.81      0.40       379\n",
      "\n",
      "avg / total       0.87      0.69      0.74      2954\n",
      "\n",
      "[[1736  839]\n",
      " [  72  307]]\n",
      "Kappa score  ::  0.25991218261352156\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[2408  167]\n",
      " [ 250  129]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.94      0.92      2575\n",
      "          1       0.44      0.34      0.38       379\n",
      "\n",
      "avg / total       0.85      0.86      0.85      2954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 4: Beta > 0.05 vs beta < 0.01\n",
    "\"\"\"\n",
    "\n",
    "df4=df.replace([np.inf, -np.inf], np.nan)\n",
    "df4=df4.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df4 = df4[(df4[82] >= 0.05) | (df4[82] <= 0.01)]\n",
    "df4[82] = (df4[82] > 0.05).astype(int)\n",
    "X = df4.iloc[:,0:77]\n",
    "y = df4[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9854249479462427\n",
      "Test Accuracy  ::  0.6263486655309484\n",
      " Confusion matrix  [[644 280]\n",
      " [378 459]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.70      0.66       924\n",
      "          1       0.62      0.55      0.58       837\n",
      "\n",
      "avg / total       0.63      0.63      0.62      1761\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.6643952299829642\n",
      "0.7458158130201141\n",
      "[[524 400]\n",
      " [191 646]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.70      0.66       924\n",
      "          1       0.62      0.55      0.58       837\n",
      "\n",
      "avg / total       0.63      0.63      0.62      1761\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.5667234525837592\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.70      0.66       924\n",
      "          1       0.62      0.55      0.58       837\n",
      "\n",
      "avg / total       0.63      0.63      0.62      1761\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.6416808631459399\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.73      0.68       924\n",
      "          1       0.65      0.54      0.59       837\n",
      "\n",
      "avg / total       0.64      0.64      0.64      1761\n",
      "\n",
      "The optimal number of neighbors is 7\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.65      0.65       924\n",
      "          1       0.62      0.62      0.62       837\n",
      "\n",
      "avg / total       0.64      0.64      0.64      1761\n",
      "\n",
      "[[602 322]\n",
      " [315 522]]\n",
      "Kappa score  ::  0.2750627671846373\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[673 251]\n",
      " [357 480]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.73      0.69       924\n",
      "          1       0.66      0.57      0.61       837\n",
      "\n",
      "avg / total       0.65      0.65      0.65      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 5: Pure integer problems. Beta > 0\n",
    "\"\"\"\n",
    "df_new = pd.concat(List1)\n",
    "df_new=df_new.drop(df_new.columns[0],axis=1)\n",
    "df5=df_new.replace([np.inf, -np.inf], np.nan)\n",
    "df5=df5.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df5[82] = (df5[82] > 0).astype(int)\n",
    "X = df5.iloc[:,0:77]\n",
    "y = df5[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9854249479462427\n",
      "Test Accuracy  ::  0.892106757524134\n",
      " Confusion matrix  [[1556   24]\n",
      " [ 166   15]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      1580\n",
      "          1       0.38      0.08      0.14       181\n",
      "\n",
      "avg / total       0.85      0.89      0.86      1761\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.8972174900624645\n",
      "0.8351667948807608\n",
      "[[1580    0]\n",
      " [ 181    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      1580\n",
      "          1       0.38      0.08      0.14       181\n",
      "\n",
      "avg / total       0.85      0.89      0.86      1761\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.8977853492333902\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      1580\n",
      "          1       0.38      0.08      0.14       181\n",
      "\n",
      "avg / total       0.85      0.89      0.86      1761\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.8949460533787621\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.94      1580\n",
      "          1       0.00      0.00      0.00       181\n",
      "\n",
      "avg / total       0.80      0.89      0.85      1761\n",
      "\n",
      "The optimal number of neighbors is 40\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.64      0.77      1580\n",
      "          1       0.20      0.78      0.32       181\n",
      "\n",
      "avg / total       0.88      0.66      0.73      1761\n",
      "\n",
      "[[1017  563]\n",
      " [  39  142]]\n",
      "Kappa score  ::  0.18766838615946235\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[1513   67]\n",
      " [ 147   34]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.96      0.93      1580\n",
      "          1       0.34      0.19      0.24       181\n",
      "\n",
      "avg / total       0.85      0.88      0.86      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 6: Pure integer problems. Beta > 0.05\n",
    "\"\"\"\n",
    "df_new = pd.concat(List1)\n",
    "df_new=df_new.drop(df_new.columns[0],axis=1)\n",
    "df5=df_new.replace([np.inf, -np.inf], np.nan)\n",
    "df5=df5.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df5[82] = (df5[82] > 0.05).astype(int)\n",
    "X = df5.iloc[:,0:77]\n",
    "y = df5[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9827298050139276\n",
      "Test Accuracy  ::  0.8852367688022285\n",
      " Confusion matrix  [[1577   27]\n",
      " [ 179   12]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      1604\n",
      "          1       0.31      0.06      0.10       191\n",
      "\n",
      "avg / total       0.84      0.89      0.85      1795\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.8935933147632312\n",
      "0.8430200676319672\n",
      "[[1604    0]\n",
      " [ 191    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      1604\n",
      "          1       0.31      0.06      0.10       191\n",
      "\n",
      "avg / total       0.84      0.89      0.85      1795\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.8924791086350975\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      1604\n",
      "          1       0.31      0.06      0.10       191\n",
      "\n",
      "avg / total       0.84      0.89      0.85      1795\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.8935933147632312\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94      1604\n",
      "          1       0.50      0.02      0.03       191\n",
      "\n",
      "avg / total       0.85      0.89      0.85      1795\n",
      "\n",
      "The optimal number of neighbors is 16\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.64      0.77      1604\n",
      "          1       0.22      0.84      0.34       191\n",
      "\n",
      "avg / total       0.89      0.66      0.72      1795\n",
      "\n",
      "[[1019  585]\n",
      " [  30  161]]\n",
      "Kappa score  ::  0.2097565899252869\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[1518   86]\n",
      " [ 146   45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.95      0.93      1604\n",
      "          1       0.34      0.24      0.28       191\n",
      "\n",
      "avg / total       0.85      0.87      0.86      1795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 7: Mixed integer problems. Beta > 0\n",
    "\"\"\"\n",
    "df_new = pd.concat(List2)\n",
    "df_new=df_new.drop(df_new.columns[0],axis=1)\n",
    "df7=df_new.replace([np.inf, -np.inf], np.nan)\n",
    "df7=df7.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df7[82] = (df7[82] > 0.05).astype(int)\n",
    "X = df7.iloc[:,0:77]\n",
    "y = df7[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9879294336118849\n",
      "Test Accuracy  ::  0.8779944289693593\n",
      " Confusion matrix  [[1559   33]\n",
      " [ 186   17]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.98      0.93      1592\n",
      "          1       0.34      0.08      0.13       203\n",
      "\n",
      "avg / total       0.83      0.88      0.84      1795\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.8863509749303621\n",
      "0.82370906255415\n",
      "[[1591    1]\n",
      " [ 203    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.98      0.93      1592\n",
      "          1       0.34      0.08      0.13       203\n",
      "\n",
      "avg / total       0.83      0.88      0.84      1795\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.8874651810584958\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.98      0.93      1592\n",
      "          1       0.34      0.08      0.13       203\n",
      "\n",
      "avg / total       0.83      0.88      0.84      1795\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.8891364902506964\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94      1592\n",
      "          1       1.00      0.02      0.04       203\n",
      "\n",
      "avg / total       0.90      0.89      0.84      1795\n",
      "\n",
      "The optimal number of neighbors is 16\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.69      0.80      1592\n",
      "          1       0.23      0.74      0.35       203\n",
      "\n",
      "avg / total       0.87      0.69      0.75      1795\n",
      "\n",
      "[[1097  495]\n",
      " [  53  150]]\n",
      "Kappa score  ::  0.21949709987383859\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[1487  105]\n",
      " [ 153   50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.93      0.92      1592\n",
      "          1       0.32      0.25      0.28       203\n",
      "\n",
      "avg / total       0.84      0.86      0.85      1795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 8: Mixed integer problems. Beta > 0.05\n",
    "\"\"\"\n",
    "df_new = pd.concat(List2)\n",
    "df_new=df_new.drop(df_new.columns[0],axis=1)\n",
    "df8=df_new.replace([np.inf, -np.inf], np.nan)\n",
    "df8=df8.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df8[82] = (df8[82] > 0.05).astype(int)\n",
    "X = df8.iloc[:,0:77]\n",
    "y = df8[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9811586051743532\n",
      "Test Accuracy  ::  0.4985939257592801\n",
      " Confusion matrix  [[1080  777]\n",
      " [1006  693]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.58      0.55      1857\n",
      "          1       0.47      0.41      0.44      1699\n",
      "\n",
      "avg / total       0.50      0.50      0.50      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.49887514060742405\n",
      "0.4958487095104568\n",
      "[[ 765 1092]\n",
      " [ 690 1009]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.58      0.55      1857\n",
      "          1       0.47      0.41      0.44      1699\n",
      "\n",
      "avg / total       0.50      0.50      0.50      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.5070303712035995\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.58      0.55      1857\n",
      "          1       0.47      0.41      0.44      1699\n",
      "\n",
      "avg / total       0.50      0.50      0.50      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.4969066366704162\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.60      0.56      1857\n",
      "          1       0.47      0.38      0.42      1699\n",
      "\n",
      "avg / total       0.49      0.50      0.49      3556\n",
      "\n",
      "The optimal number of neighbors is 7\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.49      0.51      1857\n",
      "          1       0.48      0.52      0.50      1699\n",
      "\n",
      "avg / total       0.50      0.50      0.50      3556\n",
      "\n",
      "[[906 951]\n",
      " [821 878]]\n",
      "Kappa score  ::  0.004643142716060034\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[995 862]\n",
      " [945 754]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.54      0.52      1857\n",
      "          1       0.47      0.44      0.45      1699\n",
      "\n",
      "avg / total       0.49      0.49      0.49      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 9: Try permuting the label (Beta > 0) (Sanity check)\n",
    "\"\"\"\n",
    "\n",
    "df9=df.replace([np.inf, -np.inf], np.nan)\n",
    "df9=df9.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df9[82] = (df9[82] > 0).astype(int)\n",
    "X = df9.iloc[:,0:77]\n",
    "y = df9[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "y_test = np.random.permutation(y_test)\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9841582302212224\n",
      "Test Accuracy  ::  0.6619797525309337\n",
      " Confusion matrix  [[1364  477]\n",
      " [ 725  990]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.74      0.69      1841\n",
      "          1       0.67      0.58      0.62      1715\n",
      "\n",
      "avg / total       0.66      0.66      0.66      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.6856017997750281\n",
      "0.7544689079170117\n",
      "[[1134  707]\n",
      " [ 411 1304]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.74      0.69      1841\n",
      "          1       0.67      0.58      0.62      1715\n",
      "\n",
      "avg / total       0.66      0.66      0.66      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.5688976377952756\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.74      0.69      1841\n",
      "          1       0.67      0.58      0.62      1715\n",
      "\n",
      "avg / total       0.66      0.66      0.66      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.6226096737907761\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.73      0.67      1841\n",
      "          1       0.64      0.51      0.57      1715\n",
      "\n",
      "avg / total       0.62      0.62      0.62      3556\n",
      "\n",
      "The optimal number of neighbors is 5\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.61      0.63      1841\n",
      "          1       0.61      0.67      0.64      1715\n",
      "\n",
      "avg / total       0.64      0.64      0.64      3556\n",
      "\n",
      "[[1116  725]\n",
      " [ 559 1156]]\n",
      "Kappa score  ::  0.27931957245673855\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[1307  534]\n",
      " [ 639 1076]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.71      0.69      1841\n",
      "          1       0.67      0.63      0.65      1715\n",
      "\n",
      "avg / total       0.67      0.67      0.67      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 10: Dropping irrelevant features. Beta > 0\n",
    "\"\"\"\n",
    "df10=df.drop([df.columns[73],df.columns[26],df.columns[3],df.columns[46],df.columns[18],df.columns[47],df.columns[22],df.columns[55],df.columns[52],df.columns[61],df.columns[59],df.columns[58],df.columns[57],df.columns[56],df.columns[63]], axis=1)\n",
    "df10=df10.replace([np.inf, -np.inf], np.nan)\n",
    "df10=df10.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "df10[82] = (df10[82] > 0).astype(int)\n",
    "X = df10.iloc[:,0:62]\n",
    "y = df10[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9861267341582303\n",
      "Test Accuracy  ::  0.8883577052868391\n",
      " Confusion matrix  [[3115   58]\n",
      " [ 339   44]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      3173\n",
      "          1       0.43      0.11      0.18       383\n",
      "\n",
      "avg / total       0.85      0.89      0.86      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.8922947131608548\n",
      "0.831431818237923\n",
      "[[3173    0]\n",
      " [ 383    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      3173\n",
      "          1       0.43      0.11      0.18       383\n",
      "\n",
      "avg / total       0.85      0.89      0.86      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.8925759280089989\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      3173\n",
      "          1       0.43      0.11      0.18       383\n",
      "\n",
      "avg / total       0.85      0.89      0.86      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.8928571428571429\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94      3173\n",
      "          1       0.67      0.01      0.02       383\n",
      "\n",
      "avg / total       0.87      0.89      0.84      3556\n",
      "\n",
      "The optimal number of neighbors is 34\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.61      0.75      3173\n",
      "          1       0.20      0.81      0.32       383\n",
      "\n",
      "avg / total       0.88      0.63      0.70      3556\n",
      "\n",
      "[[1942 1231]\n",
      " [  73  310]]\n",
      "Kappa score  ::  0.18093143352331198\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[2991  182]\n",
      " [ 283  100]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.94      0.93      3173\n",
      "          1       0.35      0.26      0.30       383\n",
      "\n",
      "avg / total       0.85      0.87      0.86      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 11: Dropping irrelevant features. Beta > 0.05\n",
    "\"\"\"\n",
    "df11=df.drop([df.columns[73],df.columns[26],df.columns[3],df.columns[46],df.columns[18],df.columns[47],df.columns[22],df.columns[55],df.columns[52],df.columns[61],df.columns[59],df.columns[58],df.columns[57],df.columns[56],df.columns[63]], axis=1)\n",
    "df11=df11.replace([np.inf, -np.inf], np.nan)\n",
    "df11=df11.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "df11[82] = (df11[82] > 0.05).astype(int)\n",
    "X = df11.iloc[:,0:62]\n",
    "y = df11[82]\n",
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model 12: Beta > 0.05\n",
    "\"\"\"\n",
    "df12=df.replace([np.inf, -np.inf], np.nan)\n",
    "df12=df12.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "df12[82] = (df12[82] > 0.05).astype(int)\n",
    "X = df12.iloc[:,0:-1]\n",
    "y = df12[82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tungu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_percentage = 0.75\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(kind='regular')\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train)\n",
    "X_new=pd.DataFrame(X_res)\n",
    "y_new=pd.Series(y_res)\n",
    "header = list(X_train)\n",
    "X_new.columns = header\n",
    "List1 = [X_train,X_new]\n",
    "List2 = [y_train,y_new]\n",
    "X_train = pd.concat(List1)\n",
    "y_train = pd.concat(List2)\n",
    "X_train=X_train.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRandom Forest Result\n",
      "Train Accuracy ::  0.9997309839262896\n",
      "Test Accuracy  ::  0.8622047244094488\n",
      " Confusion matrix  [[2972  194]\n",
      " [ 296   94]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.94      0.92      3166\n",
      "          1       0.33      0.24      0.28       390\n",
      "\n",
      "avg / total       0.85      0.86      0.85      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tLogistic Regression Result\n",
      "0.7845894263217098\n",
      "0.8043984968495391\n",
      "[[2535  631]\n",
      " [ 135  255]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.94      0.92      3166\n",
      "          1       0.33      0.24      0.28       390\n",
      "\n",
      "avg / total       0.85      0.86      0.85      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tSVM Result\n",
      "0.8667041619797525\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.94      0.92      3166\n",
      "          1       0.33      0.24      0.28       390\n",
      "\n",
      "avg / total       0.85      0.86      0.85      3556\n",
      "\n",
      "\n",
      "\t\t\t\t\tkNN Result\n",
      "Accuracy Score :: 0.7795275590551181\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.82      0.87      3166\n",
      "          1       0.23      0.44      0.30       390\n",
      "\n",
      "avg / total       0.85      0.78      0.81      3556\n",
      "\n",
      "The optimal number of neighbors is 1\n",
      "\n",
      "\t\t\t\t\tNaive Bayes Result\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.61      0.74      3166\n",
      "          1       0.19      0.76      0.31       390\n",
      "\n",
      "avg / total       0.87      0.63      0.70      3556\n",
      "\n",
      "[[1929 1237]\n",
      " [  95  295]]\n",
      "Kappa score  ::  0.1601298798937243\n",
      "\n",
      "\t\t\t\t\tNeural Network Result\n",
      "[[2681  485]\n",
      " [ 218  172]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.85      0.88      3166\n",
      "          1       0.26      0.44      0.33       390\n",
      "\n",
      "avg / total       0.85      0.80      0.82      3556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "trained_model=clf.fit(X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tRandom Forest Result\")\n",
    "print (\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
    "print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "print (\" Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "# Logistic regression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "predicted = model2.predict(X_test)\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tLogistic Regression Result\")\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#SVM\n",
    "model3 = svm.SVC()\n",
    "model3.fit(X_train,y_train)\n",
    "predictedSVM = model3.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tSVM Result\")\n",
    "print(metrics.accuracy_score(y_test, predictedSVM))\n",
    "print(classification_report(y_test,predictions))\n",
    "print (\"\")\n",
    "\n",
    "#kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tkNN Result\")\n",
    "print (\"Accuracy Score ::\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test,pred))\n",
    "\n",
    "myList = list(range(1,50))\n",
    "cv_scores = [] #Empty list that will hold crossvaluation score\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in myList:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores] # changing to misclassification error\n",
    "\n",
    "optimal_k = myList[MSE.index(min(MSE))]\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print (\"\")\n",
    "\n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predicted = model.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print (\"\\t\\t\\t\\t\\tNaive Bayes Result\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print (\"Kappa score  :: \", cohen_kappa_score(y_test, predicted))\n",
    "print (\"\")\n",
    "\n",
    "#Neural Network\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(78,78,78))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print (\"\\t\\t\\t\\t\\tNeural Network Result\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
